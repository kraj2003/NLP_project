{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27914e96",
   "metadata": {},
   "source": [
    "## Sentiment analysis using Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41a046fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\SB\n",
      "[nltk_data]     INFO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SB\n",
      "[nltk_data]     INFO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e47d58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "192d9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_pos_tweets=twitter_samples.strings(\"positive_tweets.json\")\n",
    "all_neg_tweets=twitter_samples.strings(\"negative_tweets.json\")\n",
    "\n",
    "train_pos=all_pos_tweets[:4000]\n",
    "test_pos=all_pos_tweets[4000:]\n",
    "train_neg=all_neg_tweets[:4000]\n",
    "test_neg=all_neg_tweets[4000:]\n",
    "\n",
    "train_x=train_pos+train_neg\n",
    "test_x=test_pos+test_neg\n",
    "\n",
    "train_y=np.append(np.ones(len(train_pos)),np.zeros(len(train_neg)))\n",
    "test_y=np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7664d00",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86713be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "import numpy as np # Library for linear algebra and math utils\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "378949ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happu', 'see']\n"
     ]
    }
   ],
   "source": [
    "custom=\"@khushi i am happu to see you! i am http://chapagain.com.np\"\n",
    "\n",
    "# process tweet\n",
    "print(process_tweets(custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80b75d",
   "metadata": {},
   "source": [
    "### building freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65e31a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(tweets,ys):\n",
    "    result={}\n",
    "    for y,tweet in zip(ys,tweets):\n",
    "        for word in process_tweets(tweet):\n",
    "            pair=(word,y)\n",
    "            \n",
    "            if pair in result:\n",
    "                result[pair]+=1\n",
    "                \n",
    "            else:\n",
    "                result[pair]=1\n",
    "                \n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "053737ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1,\n",
       " ('good', 0): 1,\n",
       " ('well', 0): 1,\n",
       " ('done', 0): 1,\n",
       " ('happi', 0): 2,\n",
       " ('see', 0): 1}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=['i happy','you are good','well done','happy to see you','happy for you']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(tweet,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc583b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e8d4a",
   "metadata": {},
   "source": [
    "### training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "640b4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs=count_tweets(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28e2007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(freqs,train_x,train_y):\n",
    "    \n",
    "    loglikelihood={}\n",
    "    logprior=0\n",
    "    \n",
    "    vocab=set([pair[0] for pair in freqs.keys()])\n",
    "#     vocab=set([pair[0] for pair in freqs.keys()])\n",
    "    V=len(vocab)\n",
    "    \n",
    "    V_pos=V_neg=N_pos=N_neg=0\n",
    "    \n",
    "    for pair in freqs.keys():\n",
    "        if pair[1]>0:\n",
    "            V_pos+=1\n",
    "            N_pos=freqs[pair]\n",
    "        else:\n",
    "            V_neg+=1\n",
    "            N_neg=freqs[pair]\n",
    "    \n",
    "    for word in vocab:\n",
    "        freq_pos=lookup(freqs,word,1)\n",
    "        freq_neg=lookup(freqs,word,0)\n",
    "        \n",
    "        p_pos=(freq_pos+1)/(N_pos+V)\n",
    "        p_neg=(freq_neg+1)/(N_neg+V)\n",
    "        \n",
    "        \n",
    "        loglikelihood[word]=np.log(p_pos/p_neg)\n",
    "    \n",
    "    D=len(train_x)\n",
    "    \n",
    "    D_pos=len(list(filter(lambda x: x>0,train_y)))\n",
    "    D_neg=len(list(filter(lambda x: x<=0,train_y)))\n",
    "    \n",
    "    logprior=np.log(D_pos)-np.log(D_neg)\n",
    "    \n",
    "    \n",
    "    return logprior, loglikelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ea4bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9161\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = training(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a54ee4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8f52",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80c93918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    \n",
    "    word_l=process_tweets(tweet)\n",
    "    \n",
    "    p=0\n",
    "    \n",
    "    p+=logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p+=loglikelihood[word]\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87aa1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.5686159179138455\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79e4bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_y,test_x,log_prior,loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    accuracy=0\n",
    "    y_hats=[]\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet,logprior,loglikelihood)>0:\n",
    "            y_hat_i=1\n",
    "        else:\n",
    "            y_hat_i=0\n",
    "        \n",
    "        y_hats.append(y_hat_i)\n",
    "        \n",
    "    error=np.mean(np.absolute(y_hats-test_y))\n",
    "    \n",
    "    accuracy=1-error\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b259b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9955\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes( test_y,test_x, logprior, loglikelihood)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
